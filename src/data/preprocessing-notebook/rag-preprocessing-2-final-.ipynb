{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13749519,"sourceType":"datasetVersion","datasetId":8746356}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install faiss-cpu sentence-transformers wikipedia beautifulsoup4 pymupdf \\\n             pytesseract easyocr \"numpy<2.0\" langchain-text-splitters -q\n\n!pip uninstall -y protobuf -q\n!pip install protobuf==4.25.3 -q\n\nimport nltk\nnltk.download(\"punkt\", quiet=True)\nnltk.download(\"punkt_tab\", quiet=True)\n\nprint(\"\\n\\n--- All packages installed and NLTK imported successfully! ---\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:27:01.368636Z","iopub.execute_input":"2025-11-16T04:27:01.369390Z","iopub.status.idle":"2025-11-16T04:28:45.153109Z","shell.execute_reply.started":"2025-11-16T04:27:01.369364Z","shell.execute_reply":"2025-11-16T04:28:45.152278Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n\n--- All packages installed and NLTK imported successfully! ---\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport shutil\nimport io\nimport time\nimport json\nimport re\nimport hashlib\nimport unicodedata\nimport html\nfrom pathlib import Path\nfrom functools import partial\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom urllib.parse import quote, unquote\n\n\nimport numpy as np\nimport faiss\nimport torch\nimport requests\nimport wikipedia\nimport pymupdf\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup, Tag\nfrom sentence_transformers import SentenceTransformer\nfrom transformers import AutoTokenizer\nfrom langchain_text_splitters import (\n    RecursiveCharacterTextSplitter,\n    SentenceTransformersTokenTextSplitter,\n)\n\n# OCR backends\ntry:\n    import pytesseract\n    _tesseract_available = True\nexcept ImportError:\n    _tesseract_available = False\n\ntry:\n    import easyocr\n    _easyocr_available = True\nexcept ImportError:\n    _easyocr_available = False\n\n# Suppress TensorFlow warning if present\nos.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n\nprint(\"--- Imports complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:28:52.061532Z","iopub.execute_input":"2025-11-16T04:28:52.061950Z","iopub.status.idle":"2025-11-16T04:29:27.051420Z","shell.execute_reply.started":"2025-11-16T04:28:52.061924Z","shell.execute_reply":"2025-11-16T04:29:27.050776Z"}},"outputs":[{"name":"stderr","text":"2025-11-16 04:29:05.682844: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1763267345.883557      48 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1763267345.934807      48 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"--- Imports complete ---\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class Constants:\n    TESSERACT_PY_AVAILABLE = _tesseract_available\n    EASYOCR_AVAILABLE = _easyocr_available\n\n    EMBED_MODEL_NAME = \"intfloat/multilingual-e5-large-instruct\"\n    EMBED_BATCH_SIZE = 32\n\n    CHUNK_MAX_TOKENS = 300\n    CHUNK_TOKEN_OVERLAP = 50\n    EMBED_DTYPE = \"float32\"\n\n    # OCR\n    OCR_DPI = 250\n    USE_TESSERACT_AUTO = True     # Use Tesseract if available, default EasyOCR\n    TESSERACT_LANGS = \"vie\"       #\n    #\n    OCR_WORKERS = max(1, min(4, (os.cpu_count() or 2) - 1)) # Number of OCR worker processes\n    DOWNSCALE_MAX_WIDTH = 1200    # Max width (px) to downscale images before OCR\n    PAGE_RENDER_BATCH = 32      # Page render batch size for memory control\n    \n    # File names\n    FAISS_INDEX_FILE = \"faiss.index\"\n    CHUNKS_FILE = \"chunks.jsonl\"\n    EMBEDDINGS_FILE = \"embeddings.npy\"\n\nprint(\"--- Constants defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:48.549889Z","iopub.execute_input":"2025-11-16T04:30:48.550502Z","iopub.status.idle":"2025-11-16T04:30:48.555620Z","shell.execute_reply.started":"2025-11-16T04:30:48.550479Z","shell.execute_reply":"2025-11-16T04:30:48.554796Z"}},"outputs":[{"name":"stdout","text":"--- Constants defined ---\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"class Utils:\n    @staticmethod\n    def sha1(text: str, length: int = 12) -> str:\n        if not isinstance(text, str):\n            text = str(text)\n        h = hashlib.sha1(text.encode(\"utf-8\")).hexdigest()\n        return h[:length]\n\n    @staticmethod\n    def slugify(text: str) -> str:\n        text = unicodedata.normalize(\"NFKD\", text)\n        text = \"\".join(ch for ch in text if not unicodedata.combining(ch))\n        text = re.sub(r\"[^0-9a-zA-Z]+\", \"-\", text).strip(\"-\")\n        return text.lower() or \"doc\"\n\n    @staticmethod\n    def normalize_vi_text(text: str) -> str:\n        if not isinstance(text, str):\n            return \"\"\n        # Basic cleanup: unescape HTML, normalize spaces\n        text = html.unescape(text)\n        text = text.replace(\"\\u00a0\", \" \")\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n    @staticmethod\n    def clean_ocr_text(text: str) -> str:\n        text = re.sub(r\"(\\w)-\\n(\\w)\", r\"\\1\\2\", text)\n        text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)\n        text = re.sub(r\"\\s+\", \" \", text).strip()\n        return text\n\nprint(\"--- Utils defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:29:55.883786Z","iopub.execute_input":"2025-11-16T04:29:55.884086Z","iopub.status.idle":"2025-11-16T04:29:55.891188Z","shell.execute_reply.started":"2025-11-16T04:29:55.884066Z","shell.execute_reply":"2025-11-16T04:29:55.890253Z"}},"outputs":[{"name":"stdout","text":"--- Utils defined ---\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"class Chunker:\n\n    @staticmethod\n    def _emit_chunk(\n        chunks: list,\n        cur_text: str,\n        chunk_id: int,\n        pinfo: dict,\n        extra_meta: dict | None = None,\n    ) -> int:\n        if not cur_text:\n            return chunk_id\n\n        fingerprint = Utils.sha1(cur_text)\n\n        # Preserve your original metadata layout\n        chunk_meta = {\n            \"id\": f\"chunk_{chunk_id}\",\n            \"doc_id\": pinfo.get(\"title\", pinfo.get(\"source\", \"doc\")),\n            \"source\": pinfo.get(\"source\"),\n            \"page\": pinfo.get(\"page\", 1),\n            \"text\": cur_text,\n            \"image_url\": pinfo.get(\"image_url\"),\n            \"url\": pinfo.get(\"url\"),\n            \"hash\": fingerprint,\n            \"iucn_text\": pinfo.get(\"iucn_text\"),\n            \"iucn_code\": pinfo.get(\"iucn_code\"),\n            \"vn_redbook_code\": pinfo.get(\"vn_redbook_code\"),\n            \"vn_redbook_text\": pinfo.get(\"vn_redbook_text\"),\n        }\n\n        if extra_meta:\n            chunk_meta.update(extra_meta)\n\n        chunks.append(chunk_meta)\n        return chunk_id + 1\n\n    @staticmethod\n    def make_chunks(\n        pages: list,\n        strategy: str = \"sentences\",\n        max_tokens: int = Constants.CHUNK_MAX_TOKENS,\n        overlap_tokens: int = Constants.CHUNK_TOKEN_OVERLAP,\n        model_name: str = Constants.EMBED_MODEL_NAME,\n    ) -> list:\n        print(f\"[chunker] Loading tokenizer: {model_name}\")\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(model_name)\n        except Exception as e:\n            print(f\"FATAL: Could not load tokenizer {model_name}. Error: {e}\")\n            return []\n\n        from langchain_core.documents import Document\n\n        langchain_docs = [\n            Document(page_content=p[\"text\"], metadata=p)\n            for p in pages\n            if p.get(\"text\", \"\").strip()\n        ]\n\n        if strategy == \"sentences\":\n            splitter = SentenceTransformersTokenTextSplitter(\n                model_name=model_name,\n                chunk_overlap=overlap_tokens,\n                tokens_per_chunk=max_tokens,\n            )\n            final_docs = splitter.split_documents(langchain_docs)\n        else:\n            if strategy == \"wiki_sections\":\n                separators = [\"\\n== \", \"\\n=== \", \"\\n\\n\", \"\\n\", \". \", \" \"]\n            else:\n                separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n\n            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n                tokenizer,\n                chunk_size=max_tokens,\n                chunk_overlap=overlap_tokens,\n                separators=separators,\n                strip_whitespace=True,\n                add_start_index=True,  # gives you start_index metadata\n            )\n            final_docs = splitter.split_documents(langchain_docs)\n\n        print(f\"[chunker] Đã xử lý {len(pages)} trang, tạo ra {len(final_docs)} chunks.\")\n\n        final_chunks_list = []\n        chunk_id_counter = 0\n        for d in final_docs:\n            extra_meta = {}\n            if \"start_index\" in d.metadata:\n                extra_meta[\"start_index\"] = d.metadata[\"start_index\"]\n\n            chunk_id_counter = Chunker._emit_chunk(\n                chunks=final_chunks_list,\n                cur_text=d.page_content,\n                chunk_id=chunk_id_counter,\n                pinfo=d.metadata,\n                extra_meta=extra_meta,\n            )\n\n        return final_chunks_list\n\nprint(\"--- Chunker class defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:29:58.356409Z","iopub.execute_input":"2025-11-16T04:29:58.357182Z","iopub.status.idle":"2025-11-16T04:29:58.367467Z","shell.execute_reply.started":"2025-11-16T04:29:58.357154Z","shell.execute_reply":"2025-11-16T04:29:58.366779Z"}},"outputs":[{"name":"stdout","text":"--- Chunker class defined ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class Deduplicator:\n    @staticmethod\n    def dedupe_chunks(chunks: list, existing_hashes: set | None = None) -> tuple:\n        seen = set(existing_hashes) if existing_hashes else set()\n        unique_chunks = []\n        added_hashes = set()\n        for ch in chunks:\n            h = ch.get(\"hash\")\n            if h and h not in seen:\n                seen.add(h)\n                unique_chunks.append(ch)\n                added_hashes.add(h)\n        return unique_chunks, added_hashes\n\nprint(\"--- Deduplicator class defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:01.231324Z","iopub.execute_input":"2025-11-16T04:30:01.231874Z","iopub.status.idle":"2025-11-16T04:30:01.237138Z","shell.execute_reply.started":"2025-11-16T04:30:01.231850Z","shell.execute_reply":"2025-11-16T04:30:01.236258Z"}},"outputs":[{"name":"stdout","text":"--- Deduplicator class defined ---\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class Embedder:\n    @staticmethod\n    def embed_chunks(\n        chunks: list,\n        model_name: str = Constants.EMBED_MODEL_NAME,\n        batch_size: int = Constants.EMBED_BATCH_SIZE,\n    ) -> np.ndarray:\n        if not chunks:\n            dim = 1024  # e5-large-instruct\n            return np.zeros((0, dim), dtype=Constants.EMBED_DTYPE)\n\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        print(f\"[embed] Using device: {device}\")\n\n        model = SentenceTransformer(model_name, device=device)\n\n        print(\"[embed] Applying 'passage: ' prefix for instruct model.\")\n        texts = [f\"passage: {Utils.normalize_vi_text(c['text'])}\" for c in chunks]\n\n        embeddings = model.encode(\n            texts,\n            batch_size=batch_size,\n            convert_to_numpy=True,\n            show_progress_bar=True,\n            normalize_embeddings=True,\n        )\n        return embeddings.astype(Constants.EMBED_DTYPE)\n\nprint(\"--- Embedder class defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:02.823678Z","iopub.execute_input":"2025-11-16T04:30:02.824419Z","iopub.status.idle":"2025-11-16T04:30:02.830442Z","shell.execute_reply.started":"2025-11-16T04:30:02.824387Z","shell.execute_reply":"2025-11-16T04:30:02.829702Z"}},"outputs":[{"name":"stdout","text":"--- Embedder class defined ---\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"class Indexer:\n    @staticmethod\n    def build_faiss(embeddings: np.ndarray) -> faiss.Index:\n        if embeddings.shape[0] == 0:\n            dim = 1024\n            return faiss.IndexFlatIP(dim)\n        dim = embeddings.shape[1]\n        index = faiss.IndexFlatIP(dim)\n        index.add(embeddings)\n        return index\n\n    @staticmethod\n    def save_index(index: \"faiss.Index\", path: str):\n        faiss.write_index(index, path)\n\n    @staticmethod\n    def load_index(path: str) -> \"faiss.Index\":\n        return faiss.read_index(path)\n\nprint(\"--- Indexer class defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:05.222492Z","iopub.execute_input":"2025-11-16T04:30:05.223183Z","iopub.status.idle":"2025-11-16T04:30:05.228708Z","shell.execute_reply.started":"2025-11-16T04:30:05.223141Z","shell.execute_reply":"2025-11-16T04:30:05.227988Z"}},"outputs":[{"name":"stdout","text":"--- Indexer class defined ---\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"class Ingestion:\n    _worker_easy_reader = None\n\n    # -----------------------------\n    # WIKIPEDIA HELPERS\n    # -----------------------------\n    @staticmethod\n    def page_url_from_title(title: str, lang: str = \"vi\") -> str:\n        safe_title = title.replace(\" \", \"_\")\n        return f\"https://{lang}.wikipedia.org/wiki/{quote(safe_title)}\"\n\n    @staticmethod\n    def extract_first_column_titles_from_url(\n        main_url: str,\n        max_titles: int | None = None\n    ) -> list[dict]:\n        headers = {\"User-Agent\": \"Mozilla/5.0 (RAG-bot/1.0)\"}\n        resp = requests.get(main_url, headers=headers, timeout=30)\n        resp.raise_for_status()\n\n        soup = BeautifulSoup(resp.content, \"html.parser\")\n\n        rows = []\n        seen_titles = set()\n\n        tables = soup.select(\"table.wikitable\")\n        for table in tables:\n            for row in table.select(\"tr\"):\n                cells = row.find_all([\"th\", \"td\"])\n                if len(cells) < 1:\n                    continue\n\n                # column 1: animal name\n                a_tag = cells[0].find(\"a\", href=True)\n                if not a_tag:\n                    continue\n\n                href = a_tag[\"href\"]\n                if not href.startswith(\"/wiki/\") or \"redlink=1\" in href:\n                    continue\n\n                title = unquote(href.split(\"/wiki/\", 1)[-1]).replace(\"_\", \" \")\n                if not title or title in seen_titles:\n                    continue\n\n                seen_titles.add(title)\n\n                # column 3: VN Red Book code on the Red List page\n                vn_code = None\n                if len(cells) >= 3:\n                    vn_code_raw = cells[2].get_text(\" \", strip=True)\n                    vn_code_raw = vn_code_raw.strip()\n                    if vn_code_raw:\n                        vn_code = vn_code_raw\n\n                row_info = {\n                    \"title\": title,\n                    \"vn_redbook_code\": vn_code,\n                    \"vn_redbook_text\": vn_code,\n                }\n                rows.append(row_info)\n\n                if max_titles and len(rows) >= max_titles:\n                    break\n            if max_titles and len(rows) >= max_titles:\n                break\n\n        return rows\n\n    # -----------------------------\n    # HTML → CLEAN TEXT\n    # -----------------------------\n    @staticmethod\n    def _get_clean_text_from_html(html: str) -> str:\n        soup = BeautifulSoup(html, \"html.parser\")\n\n        # Find the main content body\n        body = soup.find(\"div\", {\"id\": \"bodyContent\"}) or soup\n\n        # 1. REMOVE JUNK ELEMENTS FIRST\n        junk_selectors = [\n            \"table.infobox\",\n            \"div.navbox\",\n            \"div.thumb\",\n            \"div.reflist\",\n            \"ol.references\",\n            \"div.mw-references-wrap\",\n            \"div#catlinks\",\n            \"div.hatnote\",\n            \"span.mw-editsection\",\n            \"div.vertical-navbox\",\n            \"table.vertical-navbox\",\n            \"table.navbox\",\n            \"table.wikitable\"\n            \"div#mw-navigation\",\n            \"div#footer\",\n        ]\n        for selector in junk_selectors:\n            for el in body.select(selector):\n                el.decompose()\n        STOP_HEADLINES = {\n            \"chú thích\",\n            \"tham khảo\",\n            \"liên kết ngoài\",\n            \"danh mục sách đỏ\",\n            \"tài liệu tham khảo\",\n            \"xem thêm\",\n        }\n\n        texts = []\n        for el in body.find_all([\"p\", \"li\", \"h2\", \"h3\", \"h4\"], recursive=True):\n            if el.name in (\"h2\", \"h3\", \"h4\"):\n                headline = el.get_text(\" \", strip=True).lower()\n                if any(stop_word in headline for stop_word in STOP_HEADLINES):\n                    break\n\n            # If it's a text element, get the text\n            elif el.name in (\"p\", \"li\"):\n                txt = el.get_text(\" \", strip=True)\n                lower = txt.lower()\n                if lower.startswith(\"tài liệu dẫn:\"):\n                    continue\n                generic_law = (\n                    \"danh mục cấm\" in lower\n                    or \"luật các loài động vật hoang dã\" in lower\n                    or \"luật các loài hoang dã\" in lower\n                    or \"cấm săn bắt\" in lower\n                )\n                if generic_law and \"việt nam\" not in lower:\n                    continue\n                if txt and len(txt.split()) > 3:\n                    texts.append(txt)\n\n        return \"\\n\\n\".join(texts)\n\n    # -----------------------------\n    # LOW-LEVEL HTTP\n    # -----------------------------\n    @staticmethod\n    def _fetch_page_html(url: str) -> str | None:\n        headers = {\"User-Agent\": \"Mozilla/5.0 (RAG-bot/1.0)\"}\n        try:\n            resp = requests.get(url, headers=headers, timeout=30)\n            resp.raise_for_status()\n            return resp.text\n        except Exception as e:\n            print(f\"[Ingestion] Failed to fetch URL {url}: {e}\")\n            return None\n\n    @staticmethod\n    def _extract_first_image_url_from_html(html_content: str, lang: str = \"vi\") -> str | None:\n\n        try:\n            soup = BeautifulSoup(html_content, \"html.parser\")\n\n            # 1) Prefer infobox\n            infobox = soup.find(\"table\", class_=\"infobox\")\n            img = None\n            if infobox:\n                img = infobox.find(\"img\")\n\n            # 2) Fallback: any image in content\n            if img is None:\n                img = soup.find(\"img\")\n\n            if not img or not img.get(\"src\"):\n                return None\n\n            src = img[\"src\"]\n            if src.startswith(\"//\"):\n                return \"https:\" + src\n            if src.startswith(\"/\"):\n                return f\"https://{lang}.wikipedia.org{src}\"\n            return src\n        except Exception as e:\n            print(f\"[Ingestion] Failed to parse image from HTML: {e}\")\n            return None\n\n    @staticmethod\n    def _fetch_page_wikitext_by_title(title: str, lang: str = \"vi\") -> str | None:\n\n        api_url = f\"https://{lang}.wikipedia.org/w/api.php\"\n        headers = {\"User-Agent\": \"Mozilla/5.0 (RAG-bot/1.0)\"}\n        params = {\n            \"action\": \"parse\",\n            \"page\": title,\n            \"prop\": \"wikitext\",\n            \"format\": \"json\",\n            \"formatversion\": 2,\n        }\n        try:\n            resp = requests.get(api_url, params=params, headers=headers, timeout=30)\n            resp.raise_for_status()\n            data = resp.json()\n            return data.get(\"parse\", {}).get(\"wikitext\", None)\n        except Exception as e:\n            print(f\"[Ingestion] Failed to fetch wikitext for {title}: {e}\")\n            return None\n\n    @staticmethod\n    def _extract_iucn_from_wikitext(wikitext: str) -> dict | None:\n        if not wikitext:\n            return None\n\n        text = wikitext.replace(\"\\r\\n\", \"\\n\")\n\n        status_code = None\n        system = None\n\n        # 1) Template form: | status = {{LC}} or {{VU|...}}\n        m = re.search(\n            r\"\\|\\s*status\\s*=\\s*\\{\\{\\s*([^}|]+)\",\n            text,\n            flags=re.IGNORECASE,\n        )\n        if m:\n            status_code = m.group(1).strip()\n        else:\n            # 2) Simple form: | status = LC\n            m = re.search(\n                r\"\\|\\s*status\\s*=\\s*([A-Za-z0-9\\.]+)\",\n                text,\n                flags=re.IGNORECASE,\n            )\n            if m:\n                status_code = m.group(1).strip()\n\n        # status_system: usually \"iucn3.1\"\n        m = re.search(\n            r\"\\|\\s*status_system\\s*=\\s*([^\\n|]+)\",\n            text,\n            flags=re.IGNORECASE,\n        )\n        if m:\n            system = m.group(1).strip()\n\n        # If we found nothing, return None\n        if not status_code and not system:\n            return None\n\n        return {\n            \"iucn_global_code\": status_code,\n            \"iucn_global_system\": system,\n        }\n\n    # -----------------------------\n    # WIKIPEDIA PAGE FETCHING\n    # -----------------------------\n    @staticmethod\n    def fetch_wikipedia_titles(titles: list[str], lang: str = \"vi\") -> list[dict]:\n        pages = []\n        for title in titles:\n            url = Ingestion.page_url_from_title(title, lang=lang)\n\n            # 1) HTML → cleaned text + image\n            html_content = Ingestion._fetch_page_html(url)\n            if not html_content:\n                continue\n\n            # Use the new, robust scraper\n            text = Ingestion._get_clean_text_from_html(html_content)\n            image_url = Ingestion._extract_first_image_url_from_html(html_content, lang=lang)\n\n            # 2) Wikitext → IUCN info\n            wikitext = Ingestion._fetch_page_wikitext_by_title(title, lang=lang)\n            iucn_meta = Ingestion._extract_iucn_from_wikitext(wikitext) if wikitext else None\n\n            iucn_code = None\n            iucn_text = None\n            if iucn_meta and iucn_meta.get(\"iucn_global_code\"):\n                iucn_code = iucn_meta[\"iucn_global_code\"]\n                system = iucn_meta.get(\"iucn_global_system\") or \"IUCN\"\n                iucn_text = f\"{iucn_code} ({system})\"\n\n            page_info = {\n                \"id\": f\"wiki_{Utils.sha1(url)}\",\n                \"title\": title,\n                \"source\": url,\n                \"url\": url,\n                \"page\": 1,\n                \"text\": text,\n                \"image_url\": image_url,\n                \"iucn_code\": iucn_code,\n                \"iucn_text\": iucn_text,\n            }\n            pages.append(page_info)\n\n        return pages\n    # ---------------------------------\n    # --- PDF / OCR METHODS (ADDED) ---\n    # ---------------------------------\n\n    @staticmethod\n    def render_page_to_png_bytes(page: 'pymupdf.Page', dpi: int = Constants.OCR_DPI) -> bytes:\n        \n        mat = pymupdf.Matrix(dpi / 72.0, dpi / 72.0) \n        pix = page.get_pixmap(matrix=mat, alpha=False) \n        return pix.tobytes(\"png\") \n\n    @staticmethod\n    def _ocr_worker_png_bytes(png_bytes: bytes, use_tesseract: bool,\n                            tesseract_langs: str, downscale_max_width: int):\n        \n        try:\n            from io import BytesIO \n            from PIL import Image as PILImage \n        except Exception as e:\n            return f\"[ocr_error] missing PIL in worker: {e}\" \n\n        # Open image from bytes\n        try:\n            img = PILImage.open(io.BytesIO(png_bytes)).convert(\"RGB\") \n        except Exception as e:\n            return f\"[ocr_error] failed to open image: {e}\" \n\n        # Downscale image\n        try:\n            w, h = img.size \n            if downscale_max_width and w > downscale_max_width: \n                new_h = int(h * (downscale_max_width / float(w))) \n                img = img.resize((downscale_max_width, new_h), PILImage.LANCZOS) \n        except Exception:\n            pass \n\n        # Try OCR with pytesseract\n        if use_tesseract and Constants.TESSERACT_PY_AVAILABLE: \n            try:\n                import pytesseract as _pt \n                config = \"--psm 6\" \n                txt = _pt.image_to_string(img, lang=tesseract_langs, config=config) \n                return Utils.clean_ocr_text(txt) \n            except Exception:\n                # If Tesseract OCR fails, fall through to EasyOCR\n                pass \n\n        # use EasyOCR if available\n        if Constants.EASYOCR_AVAILABLE: \n            try:\n                # Initialize EasyOCR reader once per process\n                if Ingestion._worker_easy_reader is None: \n                    import easyocr as _easy \n                    # Use English and Vietnamese by default\n                    Ingestion._worker_easy_reader = _easy.Reader([\"en\", \"vi\"], gpu=False) \n                np_img = np.array(img) \n                result = Ingestion._worker_easy_reader.readtext(np_img) \n                text = \"\\n\".join([r[1] for r in result]) \n                return Utils.clean_ocr_text(text) \n            except Exception as e:\n                return f\"[ocr_easy_error] {e}\" \n\n        # If no OCR backend succeeded\n        return \"[ocr_error] no ocr backend available in worker\" \n\n    @staticmethod\n    def _run_parallel_ocr(ocr_jobs: list, use_tesseract: bool,\n                        tesseract_langs: str, workers: int, downscale_max_width: int) -> list:\n        \n        pages_out = [] \n        if not ocr_jobs: \n            return pages_out \n\n        # Prepare the worker function with fixed parameters using partial\n        worker_func = partial(Ingestion._ocr_worker_png_bytes, \n                            use_tesseract=use_tesseract, \n                            tesseract_langs=tesseract_langs, \n                            downscale_max_width=downscale_max_width) \n\n        with ProcessPoolExecutor(max_workers=workers) as executor: \n            futures = {executor.submit(worker_func, png_bytes): (title, page_no) \n                    for (title, page_no, png_bytes) in ocr_jobs} \n            \n            for future in tqdm(as_completed(futures), total=len(futures), desc=\"OCR pages\", unit=\"page\"): \n                title, page_no = futures[future] \n                try:\n                    text = future.result() \n                except Exception as e:\n                    text = f\"[ocr_exception] {e}\" \n                \n                pages_out.append({ \n                    \"page\": page_no, \n                    \"text\": text if text else \"\", \n                    \"source\": \"pdf_ocr\", \n                    \"title\": title \n                })\n\n        # Sort results by document title and page number for consistency\n        pages_out.sort(key=lambda x: (x.get(\"title\", \"\"), x.get(\"page\", 0))) \n        return pages_out \n\n    @staticmethod\n    def pdf_to_pages_with_jobs(pdf_path: str, dpi: int = Constants.OCR_DPI) -> tuple:\n        \n        if not os.path.exists(pdf_path): \n            raise FileNotFoundError(f\"PDF file not found: {pdf_path}\") \n        \n        doc = pymupdf.open(pdf_path) \n        pages_with_text = [] \n        ocr_jobs = [] \n        base_title = os.path.basename(pdf_path) \n\n        for i, page in enumerate(doc, start=1): \n            page_text = page.get_text().strip() \n            if page_text: \n                pages_with_text.append({ \n                    \"page\": i, \n                    \"text\": Utils.clean_ocr_text(page_text), \n                    \"source\": \"pdf\", \n                    \"title\": base_title \n                })\n            else:\n                # Page has no text, schedule for OCR\n                png_bytes = Ingestion.render_page_to_png_bytes(page, dpi=dpi) \n                ocr_jobs.append((base_title, i, png_bytes)) \n        \n        return pages_with_text, ocr_jobs\n\nprint(\"--- Ingestion class defined ---\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:06.911619Z","iopub.execute_input":"2025-11-16T04:30:06.912203Z","iopub.status.idle":"2025-11-16T04:30:06.944489Z","shell.execute_reply.started":"2025-11-16T04:30:06.912178Z","shell.execute_reply":"2025-11-16T04:30:06.943874Z"}},"outputs":[{"name":"stdout","text":"--- Ingestion class defined ---\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class Base:\n    _embedder_model = None\n\n    # -----------------------------\n    # Embedder\n    # -----------------------------\n    @staticmethod\n    def _get_embedder(model_name: str = Constants.EMBED_MODEL_NAME):\n        if Base._embedder_model is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            print(f\"[Base] Loading embedder {model_name} on {device}\")\n            Base._embedder_model = SentenceTransformer(model_name, device=device)\n        return Base._embedder_model\n\n    @staticmethod\n    def _embed_chunks(chunks: list) -> np.ndarray:\n        if not chunks:\n            dim = 1024\n            return np.zeros((0, dim), dtype=Constants.EMBED_DTYPE)\n\n        model = Base._get_embedder(Constants.EMBED_MODEL_NAME)\n\n        texts = [f\"passage: {Utils.normalize_vi_text(c['text'])}\" for c in chunks]\n\n        embeddings = model.encode(\n            texts,\n            batch_size=Constants.EMBED_BATCH_SIZE,\n            convert_to_numpy=True,\n            show_progress_bar=True,\n            normalize_embeddings=True,\n        )\n        return embeddings.astype(Constants.EMBED_DTYPE)\n\n    # -----------------------------\n    # FAISS\n    # -----------------------------\n    @staticmethod\n    def _build_faiss_index(embeddings: np.ndarray) -> faiss.Index:\n        return Indexer.build_faiss(embeddings)\n\n    # -----------------------------\n    # Saving\n    # -----------------------------\n    @staticmethod\n    def _save_artifacts(out_dir: str, chunks: list, embeddings: np.ndarray, index: faiss.Index):\n        out_path = Path(out_dir)\n        out_path.mkdir(parents=True, exist_ok=True)\n\n        chunks_path = out_path / Constants.CHUNKS_FILE\n        emb_path = out_path / Constants.EMBEDDINGS_FILE\n        index_path = out_path / Constants.FAISS_INDEX_FILE\n\n        with chunks_path.open(\"w\", encoding=\"utf-8\") as f:\n            for ch in chunks:\n                f.write(json.dumps(ch, ensure_ascii=False) + \"\\n\")\n\n        np.save(emb_path, embeddings)\n        Indexer.save_index(index, str(index_path))\n\n        print(\"[Base] Saved:\")\n        print(f\"  - {chunks_path}\")\n        print(f\"  - {emb_path}\")\n        print(f\"  - {index_path}\")\n\n    # -----------------------------\n    # Main prepare function\n    # -----------------------------\n    @staticmethod\n    def prepare_from_pdf_paths(\n        pdf_paths: list[str],\n        wiki_titles: list[dict] | None,\n        wiki_lang: str,\n        out_dir: str,\n        params: dict,\n        force: bool = False,\n    ):\n        strategy = params.get(\"CHUNKING_STRATEGY\", \"paragraph\")\n        max_tokens = params.get(\"CHUNK_MAX_TOKENS\", Constants.CHUNK_MAX_TOKENS)\n        overlap_tokens = params.get(\"CHUNK_TOKEN_OVERLAP\", Constants.CHUNK_TOKEN_OVERLAP)\n        \n        # --- Get OCR params from constants ---\n        ocr_dpi = params.get(\"OCR_DPI\", Constants.OCR_DPI)\n        ocr_workers = params.get(\"OCR_WORKERS\", Constants.OCR_WORKERS)\n        tesseract_langs = params.get(\"TESSERACT_LANGS\", Constants.TESSERACT_LANGS)\n        downscale_width = params.get(\"DOWNSCALE_MAX_WIDTH\", Constants.DOWNSCALE_MAX_WIDTH)\n        # Check if tesseract is enabled and available\n        use_tesseract_auto = params.get(\"USE_TESSERACT_AUTO\", Constants.USE_TESSERACT_AUTO)\n        use_tesseract = use_tesseract_auto and Constants.TESSERACT_PY_AVAILABLE\n\n        out_path = Path(out_dir)\n        chunks_path = out_path / Constants.CHUNKS_FILE\n        emb_path = out_path / Constants.EMBEDDINGS_FILE\n        index_path = out_path / Constants.FAISS_INDEX_FILE\n\n        if not force and chunks_path.exists() and emb_path.exists() and index_path.exists():\n            print(\"[Base] Artifacts already exist. Skipping prepare.\")\n            index = Indexer.load_index(str(index_path))\n            embeddings = np.load(emb_path)\n            chunks = [json.loads(l) for l in chunks_path.read_text(encoding=\"utf-8\").splitlines()]\n            return chunks, embeddings, index\n\n        # This list will hold all \"pages\" (text from PDFs, text from OCR, text from Wiki)\n        pages = []\n        \n        # --- 1) PDF ingestion (NOW FUNCTIONAL) ---\n        collected_pages = []\n        ocr_jobs = []\n        for p in pdf_paths or []:\n            if not os.path.exists(p):\n                print(f\"[Base/warn] missing pdf: {p}; skipping\")\n                continue\n            \n            print(f\"[Base/pdf] Processing PDF: {p}\")\n            \n            pdf_pages, pdf_jobs = Ingestion.pdf_to_pages_with_jobs(p, dpi=ocr_dpi) \n            collected_pages.extend(pdf_pages) \n            ocr_jobs.extend(pdf_jobs) \n\n        pages.extend(collected_pages)\n        print(f\"[Base/pdf] Found {len(collected_pages)} text pages and {len(ocr_jobs)} pages needing OCR.\")\n\n        # --- 2) Run OCR jobs (NEW SECTION) ---\n        if ocr_jobs:\n            print(f\"[Base/ocr] Running OCR on {len(ocr_jobs)} pages with {ocr_workers} workers. Using Tesseract: {use_tesseract}\")\n            \n            new_pages_from_ocr = Ingestion._run_parallel_ocr(\n                ocr_jobs,\n                use_tesseract=use_tesseract,\n                tesseract_langs=tesseract_langs,\n                workers=ocr_workers,\n                downscale_max_width=downscale_width\n            )\n            pages.extend(new_pages_from_ocr)\n            print(f\"[Base/ocr] OCR complete, got {len(new_pages_from_ocr)} pages of text.\")\n\n        # --- 3) Wikipedia ingestion (Unchanged) ---\n        if wiki_titles:\n            titles = [row[\"title\"] for row in wiki_titles]\n            redbook_map = {\n                row[\"title\"]: {\n                    \"vn_redbook_code\": row.get(\"vn_redbook_code\"),\n                    \"vn_redbook_text\": row.get(\"vn_redbook_text\"),\n                }\n                for row in wiki_titles\n            }\n            wiki_pages = Ingestion.fetch_wikipedia_titles(titles, lang=wiki_lang)\n\n            for p in wiki_pages:\n                extra = redbook_map.get(p[\"title\"], {}) or {}\n                vn_code = extra.get(\"vn_redbook_code\")\n                vn_text = extra.get(\"vn_redbook_text\")\n                \n                p[\"vn_redbook_code\"] = vn_code\n                p[\"vn_redbook_text\"] = vn_text\n                \n                if not p.get(\"iucn_code\") and vn_code:\n                    p[\"iucn_code\"] = vn_code\n                    p[\"iucn_text\"] = vn_text or vn_code\n                else:\n                    p.setdefault(\"iucn_code\", None)\n                    p.setdefault(\"iucn_text\", None)\n                \n                pages.append(p)\n\n        print(f\"[Base] Total pages collected (PDF + OCR + Wiki): {len(pages)}\")\n        if not pages:\n            print(\"[Base] No pages found. Stopping.\")\n            # Return empty but valid artifacts\n            empty_embeds = np.zeros((0, 1024), dtype=Constants.EMBED_DTYPE)\n            empty_index = Base._build_faiss_index(empty_embeds)\n            return [], empty_embeds, empty_index\n\n        # --- 4) Chunking (Unchanged) ---\n        chunks = Chunker.make_chunks(\n            pages,\n            strategy=strategy,\n            max_tokens=max_tokens,\n            overlap_tokens=overlap_tokens,\n            model_name=params.get(\"EMBED_MODEL_NAME\", Constants.EMBED_MODEL_NAME),\n        )\n        print(f\"[Base] Total chunks from Chunker: {len(chunks)}\")\n\n        # --- 5) Embedding (Unchanged) ---\n        print(f\"[Base] Loading embedder: {Constants.EMBED_MODEL_NAME}\")\n        # Note: Using the class's _embed_chunks, not the standalone Embedder.\n        embeddings = Base._embed_chunks(chunks) \n\n        print(f\"[Base] Embedding shape: {embeddings.shape}\")\n\n        # --- 6) FAISS Index (Unchanged) ---\n        index = Base._build_faiss_index(embeddings)\n        print(f\"[Base] FAISS index built with {index.ntotal} vectors (dim={index.d}).\")\n\n        # --- 7) Save (Unchanged) ---\n        Base._save_artifacts(out_dir, chunks, embeddings, index)\n\n        return chunks, embeddings, index\n\nprint(\"--- Base helper defined ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:30:15.259110Z","iopub.execute_input":"2025-11-16T04:30:15.259600Z","iopub.status.idle":"2025-11-16T04:30:15.276656Z","shell.execute_reply.started":"2025-11-16T04:30:15.259575Z","shell.execute_reply":"2025-11-16T04:30:15.275886Z"}},"outputs":[{"name":"stdout","text":"--- Base helper defined ---\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"out_dir = \"/kaggle/working/data_files_paragraph\"\nredlist_url = \"https://vi.wikipedia.org/wiki/Danh_m%E1%BB%A5c_s%C3%A1ch_%C4%91%E1%BB%8F_%C4%91%E1%BB%99ng_v%E1%BA%ADt_Vi%E1%BB%87t_Nam\"\nparams = {\n    \"CHUNK_MAX_TOKENS\": 300,\n    \"CHUNK_TOKEN_OVERLAP\": 50,\n    \"EMBED_MODEL_NAME\": Constants.EMBED_MODEL_NAME,\n    \"CHUNKING_STRATEGY\": \"paragraph\",\n    \"MAX_ANIMALS\": ,\n}\n\nmax_animals = params.get(\"MAX_ANIMALS\")\n\nwiki_rows = Ingestion.extract_first_column_titles_from_url(\n    redlist_url,\n    max_titles=max_animals,\n)\n\nprint(\"--- Running PARAGRAPH Strategy ---\")\nprint(f\"Using {len(wiki_rows)} animal rows from first column.\")\nprint(\"Example rows:\", wiki_rows[:3])\n\nchunks, embeddings, index = Base.prepare_from_pdf_paths(\n    [],\n    wiki_titles=wiki_rows,\n    wiki_lang=\"vi\",\n    out_dir=out_dir,\n    force=True,\n    params=params,\n)\nif chunks:\n    print(\"Example chunk 0:\", json.dumps(chunks[0], ensure_ascii=False)[:500])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T04:31:32.725598Z","iopub.execute_input":"2025-11-16T04:31:32.726230Z","iopub.status.idle":"2025-11-16T04:35:24.520491Z","shell.execute_reply.started":"2025-11-16T04:31:32.726204Z","shell.execute_reply":"2025-11-16T04:35:24.519689Z"}},"outputs":[{"name":"stdout","text":"--- Running PARAGRAPH Strategy ---\nUsing 289 animal rows from first column.\nExample rows: [{'title': 'Ác là', 'vn_redbook_code': 'V', 'vn_redbook_text': 'V'}, {'title': 'Báo gấm', 'vn_redbook_code': 'V', 'vn_redbook_text': 'V'}, {'title': 'Báo hoa mai Đông Dương', 'vn_redbook_code': 'E', 'vn_redbook_text': 'E'}]\n[Base/pdf] Processing PDF: /kaggle/input/chunking-11/sachdongvattest.pdf\n[Base/pdf] Found 28 text pages and 0 pages needing OCR.\n[Base] Total pages collected (PDF + OCR + Wiki): 317\n[chunker] Loading tokenizer: intfloat/multilingual-e5-large-instruct\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3d50b0a0a40460283d6f1ed33d81a2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da684938c1454d35808b9d92b7518dba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56dd9dd6623c49a9822c9cf13edfe1dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d6a213d61a0443699711db8219f176a"}},"metadata":{}},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (524 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"[chunker] Đã xử lý 317 trang, tạo ra 1319 chunks.\n[Base] Total chunks from Chunker: 1319\n[Base] Loading embedder: intfloat/multilingual-e5-large-instruct\n[Base] Loading embedder intfloat/multilingual-e5-large-instruct on cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e035c6e528fc4721a7aacfd3a01de85a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/128 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e4406aa13084c35a114157514393a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"340a34c48bc64e43bcc4d29a5e65eb5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_xlm-roberta_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b1fc06d29e41409bcb955bcb470bf2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/690 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c49ceb87e0d94b6ebb1562b50c964fd6"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dff74684967b4eed8b57124eca16ce36"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77b9c8c3d8ef41d8b1c7b8cc41afc11d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/42 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41fa17b1d0464f78bbb7c320b7956bf8"}},"metadata":{}},{"name":"stdout","text":"[Base] Embedding shape: (1319, 1024)\n[Base] FAISS index built with 1319 vectors (dim=1024).\n[Base] Saved:\n  - /kaggle/working/data_files_paragraph/chunks.jsonl\n  - /kaggle/working/data_files_paragraph/embeddings.npy\n  - /kaggle/working/data_files_paragraph/faiss.index\nExample chunk 0: {\"id\": \"chunk_0\", \"doc_id\": \"sachdongvattest.pdf\", \"source\": \"pdf\", \"page\": 1, \"text\": \"1 Bảo vệ Động vật hoang dã - Hướng dẫn tích hợp vào môn Sinh học Lớp 7 Hướng dẫn tích hợp vào môn Sinh học lớp 7 Bảo vệ động vật hoang dã Đỗ Thị Thanh Huyền Phạm Phương Bình Trần Văn Quang\", \"image_url\": null, \"url\": null, \"hash\": \"3d5aa5017199\", \"iucn_text\": null, \"iucn_code\": null, \"vn_redbook_code\": null, \"vn_redbook_text\": null, \"start_index\": 0}\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"out_dir_sent = \"/kaggle/working/data_files_sentences\"\nparams_sent = {\n    \"CHUNK_MAX_TOKENS\": 300,\n    \"CHUNK_TOKEN_OVERLAP\": 20,\n    \"EMBED_MODEL_NAME\": Constants.EMBED_MODEL_NAME,\n    \"CHUNKING_STRATEGY\": \"sentences\",\n    \"MAX_ANIMALS\": 358,\n}\nwiki_rows_sent = wiki_rows  # reuse rows from Cell 11\n\nprint(\"--- Running SENTENCES Strategy ---\")\nprint(f\"Using {len(wiki_rows_sent)} animal rows from first column.\")\n\nchunks_s, embeddings_s, index_s = Base.prepare_from_pdf_paths(\n    [],\n    wiki_titles=wiki_rows_sent,\n    wiki_lang=\"vi\",\n    out_dir=out_dir_sent,\n    force=True,\n    params=params_sent,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"out_dir_wiki = \"/kaggle/working/data_files_wiki_sections\"\nparams_wiki = {\n    \"CHUNK_MAX_TOKENS\": 300,\n    \"CHUNK_TOKEN_OVERLAP\": 50,\n    \"EMBED_MODEL_NAME\": Constants.EMBED_MODEL_NAME,\n    \"CHUNKING_STRATEGY\": \"wiki_sections\",\n    \"MAX_ANIMALS\": 358,\n}\n\nwiki_rows_wiki = wiki_rows  # reuse\n\nprint(\"--- Running WIKI_SECTIONS Strategy ---\")\nprint(f\"Using {len(wiki_rows_wiki)} animal rows from first column.\")\n\nchunks_w, embeddings_w, index_w = Base.prepare_from_pdf_paths(\n    [],\n    wiki_titles=wiki_rows_wiki,\n    wiki_lang=\"vi\",\n    out_dir=out_dir_wiki,\n    force=True,\n    params=params_wiki,\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
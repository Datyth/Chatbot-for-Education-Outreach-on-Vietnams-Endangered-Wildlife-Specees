{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 13708542,
          "sourceType": "datasetVersion",
          "datasetId": 8716664
        },
        {
          "sourceId": 13727352,
          "sourceType": "datasetVersion",
          "datasetId": 8732421
        }
      ],
      "dockerImageVersionId": 31192,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebookd68f00cac7",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "import kagglehub\n",
        "riatleozia_filess_path = kagglehub.dataset_download('riatleozia/filess')\n",
        "riatleozia_chunking_2_path = kagglehub.dataset_download('riatleozia/chunking-2')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "enwtcZGMwqw8"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"transformers>=4.40.0\" sentence-transformers faiss-cpu safetensors -q\n",
        "!pip uninstall -y protobuf\n",
        "!pip install protobuf==4.25.3"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T05:35:58.819702Z",
          "iopub.execute_input": "2025-11-14T05:35:58.820477Z",
          "iopub.status.idle": "2025-11-14T05:37:20.526084Z",
          "shell.execute_reply.started": "2025-11-14T05:35:58.820435Z",
          "shell.execute_reply": "2025-11-14T05:37:20.525324Z"
        },
        "id": "7Byz1g1vwqxA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os, textwrap, pathlib, pprint\n",
        "root = \"/kaggle/input/filess\"\n",
        "for path, dirs, files in os.walk(root):\n",
        "    print(path)\n",
        "    print(\"  dirs:\", dirs)\n",
        "    print(\"  files:\", files)\n",
        "    print(\"-\" * 40)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T05:37:25.948941Z",
          "iopub.execute_input": "2025-11-14T05:37:25.949673Z",
          "iopub.status.idle": "2025-11-14T05:37:25.974716Z",
          "shell.execute_reply.started": "2025-11-14T05:37:25.949643Z",
          "shell.execute_reply": "2025-11-14T05:37:25.973784Z"
        },
        "id": "NvKUdQtZwqxB"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple\n",
        "\n",
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Config for Kaggle\n",
        "# ===========================\n",
        "INPUT_ROOT = Path(\"/kaggle/input/chunking-2\")\n",
        "\n",
        "# Writable outputs\n",
        "WORK_ROOT = Path(\"/kaggle/working\")\n",
        "KB_CONFIGS = {\n",
        "    \"sentences\": {\n",
        "        \"base_dir\": INPUT_ROOT / \"data_files_sentences\",\n",
        "        \"chunks_file\": \"chunks.jsonl\",\n",
        "        \"index_file\": \"index.faiss\",\n",
        "        \"embed_model_name\": \"intfloat/multilingual-e5-large-instruct\",\n",
        "    },\n",
        "    \"wiki_sections\": {\n",
        "        \"base_dir\": INPUT_ROOT / \"data_files_wiki_sections\",\n",
        "        \"chunks_file\": \"chunks.jsonl\",\n",
        "        \"index_file\": \"index.faiss\",\n",
        "        \"embed_model_name\": \"intfloat/multilingual-e5-large-instruct\",\n",
        "    },\n",
        "    \"paragraph\": {\n",
        "        \"base_dir\": INPUT_ROOT / \"data_files_paragraph\",\n",
        "        \"chunks_file\": \"chunks.jsonl\",\n",
        "        \"index_file\": \"index.faiss\",\n",
        "        \"embed_model_name\": \"intfloat/multilingual-e5-large-instruct\",\n",
        "    },\n",
        "}\n",
        "\n",
        "QA_PATH = INPUT_ROOT / \"sach_do_dong_vat_vietnam_qa_dataset.json\"\n",
        "OUT_DIR = WORK_ROOT / \"eval_results\"\n",
        "OUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# LLM loading (Hugging Face)\n",
        "# ===========================\n",
        "\n",
        "# You can swap this to a larger model if Kaggle GPU allows, e.g.\n",
        "# _LLM_MODEL_ID = \"Phat-Dat/Llama-3.2-8B-RLHF-DPO\"\n",
        "_LLM_MODEL_ID = \"Phat-Dat/Llama-3.2-1B-RLHF-DPO\"\n",
        "_llm_model = None\n",
        "_llm_tokenizer = None\n",
        "\n",
        "\n",
        "def get_llm():\n",
        "    \"\"\"Lazy-load HF model & tokenizer (GPU if available).\"\"\"\n",
        "    global _llm_model, _llm_tokenizer\n",
        "\n",
        "    if _llm_model is not None and _llm_tokenizer is not None:\n",
        "        return _llm_model, _llm_tokenizer\n",
        "\n",
        "    print(f\"[llm] loading model: {_LLM_MODEL_ID}\")\n",
        "    _llm_tokenizer = AutoTokenizer.from_pretrained(_LLM_MODEL_ID)\n",
        "    _llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        _LLM_MODEL_ID,\n",
        "        torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "        device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    )\n",
        "\n",
        "    if _llm_tokenizer.pad_token_id is None:\n",
        "        _llm_tokenizer.pad_token_id = _llm_tokenizer.eos_token_id\n",
        "\n",
        "    _llm_model.eval()\n",
        "\n",
        "    return _llm_model, _llm_tokenizer\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# QA loading\n",
        "# ===========================\n",
        "\n",
        "def load_qa_pairs() -> List[Dict]:\n",
        "    data = json.loads(QA_PATH.read_text(encoding=\"utf-8\"))\n",
        "    return data[\"qa_pairs\"]\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# KB loading (one chunking method)\n",
        "# ===========================\n",
        "\n",
        "def load_kb(kb_id: str) -> Tuple[faiss.Index, List[Dict], str]:\n",
        "    cfg = KB_CONFIGS[kb_id]\n",
        "    base_dir = cfg[\"base_dir\"]\n",
        "\n",
        "    chunks_path = base_dir / cfg[\"chunks_file\"]\n",
        "    index_path = base_dir / cfg[\"index_file\"]\n",
        "    embed_model_name = cfg[\"embed_model_name\"]\n",
        "\n",
        "    # load chunks\n",
        "    chunks: List[Dict] = []\n",
        "    with chunks_path.open(encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            chunks.append(json.loads(line))\n",
        "\n",
        "    # load FAISS index\n",
        "    index = faiss.read_index(str(index_path))\n",
        "\n",
        "    return index, chunks, embed_model_name\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Embedding + retrieval\n",
        "# ===========================\n",
        "\n",
        "_embedder_cache: Dict[str, SentenceTransformer] = {}\n",
        "\n",
        "\n",
        "def get_embedder(model_name: str) -> SentenceTransformer:\n",
        "    if model_name not in _embedder_cache:\n",
        "        print(f\"[embed] loading model {model_name}\")\n",
        "        _embedder_cache[model_name] = SentenceTransformer(model_name)\n",
        "    return _embedder_cache[model_name]\n",
        "\n",
        "\n",
        "def retrieve_topk(\n",
        "    index: faiss.Index,\n",
        "    embedder: SentenceTransformer,\n",
        "    question: str,\n",
        "    chunks: List[Dict],\n",
        "    top_k: int = 5,\n",
        ") -> Tuple[List[Dict], List[float]]:\n",
        "    # E5-style query prefix\n",
        "    q_text = f\"query: {question}\"\n",
        "    q_vec = embedder.encode([q_text], normalize_embeddings=True)\n",
        "    q_vec = q_vec.astype(\"float32\")\n",
        "    D, I = index.search(q_vec, top_k)\n",
        "    indices = I[0]\n",
        "    scores = D[0]\n",
        "    retrieved = [chunks[i] for i in indices]\n",
        "    return retrieved, scores.tolist()\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Text normalization + scoring\n",
        "# ===========================\n",
        "\n",
        "VN_STOPWORDS = {\n",
        "    \"và\", \"có\", \"là\", \"các\", \"một\", \"ở\", \"của\", \"trong\", \"đã\", \"được\", \"bị\",\n",
        "    \"trên\", \"đến\", \"tại\", \"không\", \"từ\", \"về\", \"vào\", \"những\", \"năm\", \"như\",\n",
        "    \"khi\", \"để\", \"cho\", \"rất\", \"rằng\", \"này\", \"kia\", \"đó\", \"này\", \"nên\",\n",
        "    \"thì\", \"ra\", \"vẫn\", \"cũng\", \"nữa\", \"cùng\", \"do\", \"vì\", \"nên\", \"nếu\",\n",
        "    \"hay\", \"hoặc\", \"v.v\", \"v.v.\",\n",
        "    \"loài\", \"động\", \"vật\", \"việt\", \"nam\", \"việt\", \"nam.\", \"sách\", \"đỏ\",\n",
        "    \"nhóm\", \"nguy\", \"cấp\", \"quý\", \"hiếm\", \"bảo\", \"tồn\", \"tình\", \"trạng\",\n",
        "    \"khoa\", \"học\", \"tên\", \"khoa\", \"học\", \"được\", \"xếp\", \"vào\", \"thuộc\",\n",
        "    \"họ\", \"giống\", \"loài\", \"đây\", \"là\", \"một\", \"trong\", \"các\", \"loài\",\n",
        "    \"vùng\", \"miền\", \"khu\", \"vực\", \"sống\", \"phân\", \"bố\",\n",
        "    \"châu\", \"á\", \"âu\", \"phi\", \"đông\", \"nam\", \"tây\", \"bắc\",\n",
        "}\n",
        "\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    text = text.strip()\n",
        "    return text\n",
        "\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Simple VN-friendly tokenization: lowercase, split on non-word chars.\n",
        "    \"\"\"\n",
        "    text = text.lower()\n",
        "    # \\w keeps letters and digits (including Vietnamese letters in most setups)\n",
        "    tokens = re.findall(r\"\\w+\", text, flags=re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens: list[str]) -> list[str]:\n",
        "    return [t for t in tokens if t not in VN_STOPWORDS and len(t) > 1]\n",
        "\n",
        "\n",
        "def f1_bow(pred_tokens: list[str], gold_tokens: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Bag-of-words F1 (like SQuAD): counts duplicates.\n",
        "    \"\"\"\n",
        "    if not pred_tokens or not gold_tokens:\n",
        "        return 0.0\n",
        "\n",
        "    from collections import Counter\n",
        "\n",
        "    pred_counts = Counter(pred_tokens)\n",
        "    gold_counts = Counter(gold_tokens)\n",
        "\n",
        "    common = 0\n",
        "    for t, g_count in gold_counts.items():\n",
        "        common += min(g_count, pred_counts.get(t, 0))\n",
        "\n",
        "    if common == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = common / sum(pred_counts.values())\n",
        "    recall = common / sum(gold_counts.values())\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "\n",
        "def exact_match(pred: str, gold: str) -> bool:\n",
        "    \"\"\"\n",
        "    Strict EM: normalized string equality.\n",
        "    \"\"\"\n",
        "    return normalize_text(pred) == normalize_text(gold)\n",
        "\n",
        "\n",
        "def smoothed_f1(pred: str, gold: str) -> tuple[float, dict]:\n",
        "    \"\"\"\n",
        "    More lenient F1:\n",
        "      - compute F1 on all tokens\n",
        "      - compute F1 on content tokens (stopwords removed)\n",
        "      - if prediction is basically the gold answer but shorter\n",
        "        (subset of content tokens or substring), boost score.\n",
        "      - final score = max(F1_all, F1_content), then possibly boosted.\n",
        "    Returns (final_f1, debug_info).\n",
        "    \"\"\"\n",
        "    pred_norm = normalize_text(pred)\n",
        "    gold_norm = normalize_text(gold)\n",
        "\n",
        "    pred_tokens_all = tokenize(pred)\n",
        "    gold_tokens_all = tokenize(gold)\n",
        "\n",
        "    pred_tokens_content = remove_stopwords(pred_tokens_all)\n",
        "    gold_tokens_content = remove_stopwords(gold_tokens_all)\n",
        "\n",
        "    f1_all = f1_bow(pred_tokens_all, gold_tokens_all)\n",
        "    f1_content = f1_bow(pred_tokens_content, gold_tokens_content)\n",
        "\n",
        "    # base: take the more generous of the two\n",
        "    base_f1 = max(f1_all, f1_content)\n",
        "\n",
        "    # If all content words in prediction are contained in gold content words,\n",
        "    # treat this as a good short answer and bump to at least 0.8.\n",
        "    pred_set = set(pred_tokens_content)\n",
        "    gold_set = set(gold_tokens_content)\n",
        "    subset_boost = False\n",
        "    if pred_set and pred_set.issubset(gold_set):\n",
        "        base_f1 = max(base_f1, 0.8)\n",
        "        subset_boost = True\n",
        "\n",
        "    # substring heuristic: if one string contains the other and they are not\n",
        "    # *too* short, treat as a near perfect match\n",
        "    substring_match = False\n",
        "    if len(pred_norm) >= 10 and len(gold_norm) >= 10:\n",
        "        if pred_norm in gold_norm or gold_norm in pred_norm:\n",
        "            substring_match = True\n",
        "\n",
        "    final_f1 = base_f1\n",
        "    if substring_match:\n",
        "        final_f1 = max(final_f1, 0.9)\n",
        "\n",
        "    # clamp to [0, 1]\n",
        "    final_f1 = max(0.0, min(1.0, final_f1))\n",
        "\n",
        "    debug = {\n",
        "        \"f1_all\": f1_all,\n",
        "        \"f1_content\": f1_content,\n",
        "        \"base_f1\": base_f1,\n",
        "        \"substring_match\": substring_match,\n",
        "        \"subset_boost\": subset_boost,\n",
        "    }\n",
        "    return final_f1, debug\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# LLM call (uses retrieved chunks)\n",
        "# ===========================\n",
        "\n",
        "def call_llm(question: str, context_chunks: List[Dict]) -> str:\n",
        "    model, tokenizer = get_llm()\n",
        "\n",
        "    # Build context from retrieved chunks\n",
        "    retrieved_context = \"\\n\\n\".join(c.get(\"text\", \"\") for c in context_chunks)\n",
        "\n",
        "    # If context is basically empty, obey the task rule explicitly\n",
        "    if not retrieved_context.strip():\n",
        "        return \"Không tìm thấy trong tài liệu.\"\n",
        "\n",
        "    # ===== system & user prompts =====\n",
        "    system_prompt = \"\"\"Bạn là trợ lý AI chuyên nghiệp, trả lời các câu hỏi về động vật trong Sách đỏ Việt Nam.\n",
        "\n",
        "NHIỆM VỤ CỐT LÕI:\n",
        "- Chỉ được phép sử dụng thông tin từ \"TÀI LIỆU CUNG CẤP\" để trả lời..\n",
        "- Tuyệt đối không thêm bất kỳ thông tin nào bên ngoài tài liệu.\n",
        "- Nếu không tìm thấy thông tin trong tài liệu để trả lời câu hỏi, hãy trả lời chính xác là: \"Không tìm thấy trong tài liệu.\"\n",
        "\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Dựa vào tài liệu sau:\n",
        "\n",
        "TÀI LIỆU CUNG CẤP:\n",
        "\\\"\\\"\\\"\n",
        "{retrieved_context}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Hãy trả lời câu hỏi sau:\n",
        "CÂU HỎI: {question}\n",
        "\"\"\"\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
        "        # Use chat template if available (LLaMA-style)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "\n",
        "        inputs = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=True,\n",
        "            return_tensors=\"pt\",   # on this HF version this returns a Tensor\n",
        "        )\n",
        "\n",
        "        # Handle both Tensor and dict (future-proof)\n",
        "        if isinstance(inputs, torch.Tensor):\n",
        "            input_ids = inputs.to(device)\n",
        "        else:\n",
        "            input_ids = inputs[\"input_ids\"].to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=False,\n",
        "                temperature=0.0,\n",
        "                top_p=1.0,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        # Cut off the prompt part\n",
        "        prompt_len = input_ids.shape[1]\n",
        "        gen_ids = outputs[0, prompt_len:]\n",
        "        answer = tokenizer.decode(gen_ids, skip_special_tokens=True).strip()\n",
        "\n",
        "    else:\n",
        "        # Fallback: no chat template, build plain prompt\n",
        "        prompt = (\n",
        "            f\"{system_prompt}\\n\\n\"\n",
        "            f\"{user_prompt}\\n\\n\"\n",
        "            \"TRẢ LỜI:\"\n",
        "        )\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=2048,\n",
        "        )\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=False,\n",
        "                temperature=0.0,\n",
        "                top_p=1.0,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "            )\n",
        "\n",
        "        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        if generated.startswith(prompt):\n",
        "            answer = generated[len(prompt):].strip()\n",
        "        else:\n",
        "            answer = generated.strip()\n",
        "\n",
        "    # Clean up leading markers like \"TRẢ LỜI:\" etc.\n",
        "    answer = re.sub(\n",
        "        r\"^(TRẢ LỜI|trả lời|assistant|trợ lý)[:：]\\s*\",\n",
        "        \"\",\n",
        "        answer,\n",
        "        flags=re.IGNORECASE,\n",
        "    ).strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Evaluate one KB\n",
        "# ===========================\n",
        "\n",
        "def evaluate_kb(kb_id: str, top_k: int = 10) -> None:\n",
        "    print(f\"=== Evaluating KB: {kb_id} ===\")\n",
        "\n",
        "    index, chunks, embed_model_name = load_kb(kb_id)\n",
        "    embedder = get_embedder(embed_model_name)\n",
        "    qa_pairs = load_qa_pairs()\n",
        "\n",
        "    results = []\n",
        "    em_scores = []\n",
        "    f1_scores = []\n",
        "    relaxed_hits = []  # F1 >= 0.3\n",
        "\n",
        "    for i, qa in enumerate(qa_pairs):\n",
        "        q = qa[\"question\"]\n",
        "        gold = qa[\"answer\"]\n",
        "\n",
        "        retrieved, scores = retrieve_topk(index, embedder, q, chunks, top_k=top_k)\n",
        "        pred = call_llm(q, retrieved)\n",
        "\n",
        "        em = exact_match(pred, gold)\n",
        "        f1, debug = smoothed_f1(pred, gold)\n",
        "\n",
        "        em_scores.append(1.0 if em else 0.0)\n",
        "        f1_scores.append(f1)\n",
        "        relaxed_hits.append(1.0 if f1 >= 0.3 else 0.0)\n",
        "\n",
        "        results.append(\n",
        "            {\n",
        "                \"id\": i,\n",
        "                \"question\": q,\n",
        "                \"gold_answer\": gold,\n",
        "                \"model_answer\": pred,\n",
        "                \"exact_match\": em,\n",
        "                \"f1\": f1,\n",
        "                \"relaxed_hit\": f1 >= 0.3,\n",
        "                \"retrieved_chunk_ids\": [c.get(\"id\") for c in retrieved],\n",
        "                \"retrieved_scores\": [float(s) for s in scores],\n",
        "                \"f1_debug\": debug,\n",
        "            }\n",
        "        )\n",
        "\n",
        "        print(f\"[{kb_id}] {i}/{len(qa_pairs)} → EM={em}, F1={f1:.3f}\")\n",
        "\n",
        "    summary = {\n",
        "        \"kb_id\": kb_id,\n",
        "        \"n_questions\": len(qa_pairs),\n",
        "        \"exact_match\": sum(em_scores) / len(em_scores) if em_scores else 0.0,\n",
        "        \"f1\": sum(f1_scores) / len(f1_scores) if f1_scores else 0.0,\n",
        "        \"relaxed_accuracy\": sum(relaxed_hits) / len(relaxed_hits) if relaxed_hits else 0.0,\n",
        "    }\n",
        "\n",
        "    out = {\n",
        "        \"summary\": summary,\n",
        "        \"results\": results,\n",
        "    }\n",
        "\n",
        "    out_path = OUT_DIR / f\"{kb_id}.json\"\n",
        "    out_path.write_text(json.dumps(out, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
        "\n",
        "    print(\n",
        "        f\"[{kb_id}] DONE. \"\n",
        "        f\"EM={summary['exact_match']:.3f}, \"\n",
        "        f\"F1={summary['f1']:.3f}, \"\n",
        "        f\"RelaxedAcc(F1>=0.3)={summary['relaxed_accuracy']:.3f}\"\n",
        "    )\n",
        "    print(f\"Saved to {out_path}\")\n",
        "\n",
        "\n",
        "# ===========================\n",
        "# Main\n",
        "# ===========================\n",
        "\n",
        "def main():\n",
        "    for kb_id in KB_CONFIGS.keys():\n",
        "        evaluate_kb(kb_id, top_k=10)\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-11-14T05:37:28.060293Z",
          "iopub.execute_input": "2025-11-14T05:37:28.060867Z",
          "iopub.status.idle": "2025-11-14T05:40:50.430659Z",
          "shell.execute_reply.started": "2025-11-14T05:37:28.060843Z",
          "shell.execute_reply": "2025-11-14T05:40:50.429897Z"
        },
        "id": "X-4MvOS2wqxB"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}